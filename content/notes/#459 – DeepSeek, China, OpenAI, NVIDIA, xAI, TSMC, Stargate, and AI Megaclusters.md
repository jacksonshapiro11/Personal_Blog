# #459 – DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters

![](https://wsrv.nl/?url=https%3A%2F%2Flexfridman.com%2Fwordpress%2Fwp-content%2Fuploads%2Fpowerpress%2Fartwork_3000-230.png&w=100&h=100)

### Metadata

- Author: Lex Fridman Podcast
- Full Title: #459 – DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters
- Category: #podcasts



- URL: https://share.snipd.com/episode/f788fc28-5de4-4500-926d-35a10f0f1869

### Highlights

- **Three Stages of Language Model Training**
  - Pre-training: Predict the next token in a series of documents using autoregressive prediction, often using trillions of tokens from sources like Common Crawl.
  - Instruction tuning (IFT/SFT): Format the model to take questions and respond informatively, like Reddit or Stack Overflow answers.
  - Preference fine-tuning: Align model responses with human preferences using techniques like reinforcement learning from human feedback (RLHF), where models learn from pairwise comparisons of good and bad answers.
  - Reinforcement fine-tuning: Use reinforcement learning to verify model answers against true solutions, allowing models to improve accuracy in verifiable domains like math and code.
  Transcript:
  Nathan Lambert
  Yeah, so pre-training, I'm using some of the same words that really get the message across, is you're doing what is called autoregressive prediction to predict the next token in a series Of documents. This is done over standard practice is trillions of tokens. So this is a ton of data that is mostly scraped from the web. And some of DeepSeek's earlier papers, they talk about their training data being distilled for math. I shouldn't use this word yet, but taken from Common Crawl. And that's a public access that anyone listening to this could go download data from the Common Crawl website. This is a crawler that is maintained publicly. Yes, other tech companies eventually shift to their own crawler and DeepSeek likely has done this as well as most frontier labs do. But this sort of data is something that people can get started with. And you're just predicting text in a series of documents this is but can be scaled to be very efficient and there's a lot of numbers that are thrown around in ai training like how many floating Point operations or flops are used and then you can also look at how many hours of these gpus that are used and it's largely one loss function taken to a very large amount of compute usage You just you set up really efficient systems and then at the end of that you have this base model and pre-training is where there is a lot more of complexity in terms of how the process is Emerging or evolving and the different types of training losses that you will use. I think this is a lot of techniques grounded in the natural language processing literature. The oldest technique, which is still used today, is something called instruction tuning or also known as supervised fine tuning. These acronyms will be IFT or SFT. People really go back and forth throughout them. And I will probably do the same, which is where you add this formatting to the model where it knows to take a question that is like, explain the history of the Roman empire to me. And, or something you'll sort of question you'll see on Reddit or Stack Overflow. And then the model will respond in a information dense, but presentable manner. The core of that formatting is in this instruction tuning phase. And then there's two other categories of loss functions that are being used today. One I will classify as preference fine tuning. Preference fine tuning is a generalized term for what came out of reinforcement learning from human feedback, which is RLHF. This reinforcement learning from human feedback is credited as the technique that helped ChatGPT break through. It is a technique to make the responses that are nicely formatted, like these Reddit answers, more in tune with what a human would like to read. This is done by collecting pairwise preferences from actual humans out in the world to start. And now AIs are also labeling this data and we'll get into those trade-offs. And you have this kind of contrastive loss function between a good answer and a bad answer. And the model learns to pick up these trends. There's different implementation ways. You have things called reward models. You could have direct alignment algorithms. There's a lot of really specific things you can do, but all of this is about fine-tuning to human preferences. And the final stage is much newer and will link to what is done in R1 and these reasoning models is, I think, OpenAI's name for this. They had this new API in the fall, which they called the Reinforcement Fine-Tuning API. This is the idea that you use the techniques of reinforcement learning, which is a whole framework of AI. There's a deep literature here. To summarize, it's often known as trial and error learning or the subfield of AI where you're trying to make sequential decisions in a certain potentially noisy environment. There's a lot of ways we could go down that. But fine-tuning language models where they can generate an answer, and then you check to see if the answer matches the true solution. For math or code, you have an exactly correct answer for math. You can have unit tests for code. And what we're doing is we are checking the language model's work, and we're giving it multiple opportunities on the same question to see if it is right. And if you keep doing this, the models can learn to improve in verifiable domains to a great extent. It works really well. It's a newer technique in the academic literature. It's been used at frontier labs in the US that don't share every detail for multiple years. So this is the idea of using reinforcement learning with language models, and it has been taking off, especially in this DeepSeq moment.
  Lex Fridman
  And we should say that there's a lot of exciting stuff going on on the, again, across the stack. But the post-training, probably this year, there's going to be a lot of interesting developments in the post-training. We'll talk about it. I almost forgot to talk about the difference between DeepSeq v3 and R1 on the user experience side. ([Time 0:24:22](https://share.snipd.com/snip/dab36ade-c77c-4971-8df5-49faabd2df44))
- **Three Axes of AI Chips**
  - AI chips can be evaluated along three key axes: floating point operations (flops), memory bandwidth/capacity, and interconnect bandwidth.
  - US export controls initially focused on flops and interconnect but shifted to solely flops, allowing manufacturers to optimize the other two.
  - Reasoning tasks prioritize memory bandwidth and capacity over flops, unlike pre-training, which heavily relies on flops.
  - The transformer architecture's attention mechanism requires calculating relationships between words, adding another layer of computational demand.
  - Key-value cache (KV cache) is crucial for storing and accessing compressed representations of previous tokens to efficiently process context in autoregressive models.
  Transcript:
  Dylan Patel
  And so this H20 has been neutered in one way, but it's actually upgraded in other ways, right? And, you know, you could think of chips along three axes for AI, right? You know, ignoring software stack and like exact architecture, just raw specifications, there's floating point operations, right? Flops. There is memory bandwidth, i.e. In memory capacity, right? IO, right? Memory. And then there is interconnect, right? Chip to chip interconnections. All three of these are incredibly important for making AI systems, right? Because AI systems involve a lot of compute. They involve a lot of moving memory around, whether it be to memory or to other chips, right? And so these three vectors, the US initially had two of these vectors controlled and one of them not controlled, which was flops and interconnect bandwidth were initially controlled. And then they said, no, no, no, no, we're going to remove the interconnect bandwidth and just make it a very simple only flops. But now NVIDIA can now make a chip that has, okay, it's cut down on flops. No, it's, you know, it's like one third that of the H100, right? In on spec sheet paper performance for flops, you know, in real world, it's closer to like half or maybe even like 60% of it, right? But then on the other two vectors, it's just as good for interconnect bandwidth. And then for memory bandwidth and memory capacity, the H20 has more memory bandwidth and more memory capacity than the H100. Now, recently, we at our research, we cut NVIDIA's production for H20 for this year down drastically. They were going to make another 2 million of those this year, but they just canceled all the orders a couple of weeks ago. In our view, that's because we think that they think they're going to get restricted, right? Because why would they cancel all these orders for H20? Because they shipped a million of them last year. They had orders in for a couple million this year and just gone, right? For H20, B20, right? A successor to H20. And now they're all gone. Now, why would they do this, right? I think it's very clear, right? The H20 is actually better for certain tasks. And that certain task is reasoning, right? Reasoning is incredibly different than, you know, when you look at the different regimes of models, right? Pre-training is all about flops, right? It's all about flops. There's things you do, like mixture of experts that we talked about to trade off interconnect or to trade off, you know, other aspects and lower the flops and rely more on interconnect And memory. But at the end of the day, it's flops is everything, right? We talk about models in terms of like how many flops they are, right? So, so like, you know, we talk about, oh, GPT-4 is 2E25, right? Two to the 25th, you know, 25 zeros, right? Flop, right? Floating point operations. For training. For training, right? And we're talking about the restrictions for the 2E24, right? Or 25, whatever. The US has an executive order that Trump recently unsigned, but which was, hey, 1E26, once you hit that number of floating point operations, you must notify the government and you must Share your results with us. There's a level of model where the U.S. Government must be told, and that's 1E26. And so as we move forward, this is an incredibly important flop is the vector that the government has cared about historically. But the other two vectors are arguably just as important, right? And especially when we come to this new paradigm, which the world is only just learning about over the last six months, right? Reasoning.
  Lex Fridman
  And do we understand firmly which of the three dimensions is best for reasoning? So interconnect, the flops don't matter as much. Is it memory? Memory, right?
  Nathan Lambert
  Yeah. Excellent. We're going to get into technical stuff real fast.
  Dylan Patel
  There's two articles in this one that I could show, maybe graphics that might be interesting for you to pull up.
  Lex Fridman
  For the listeners, we're looking at the section of 01 inference architecture tokenomics.
  Dylan Patel
  You want to explain KVCache before we talk about this? I think it's better to...
  Nathan Lambert
  Okay, yeah. We need to go through a lot of specific technical things of transformers to make this easy for people.
  Dylan Patel
  Because it's incredibly important because this changes how models work. But I think resetting, right? Why is memory so important? It's because so far we've talked about parameter counts, right? And mixture of experts, you can change how many active parameters versus total parameters to embed more data, but have less flops. But more important, you know, another aspect of, you know, what's part of this humongous revolution in the last handful of years is the transformer, right? And the attention mechanism. Attention mechanism is that the model understands the relationships between all the words in its context, right? And that is separate from the parameters themselves, right? And that is something that you must calculate, right? How each token, right? Each word in the context length is relatively connected to each other, right? And I think, Nathan, you should explain KVCache better.
  Lex Fridman
  KVCache is one of the optimizations that enable.
  Nathan Lambert
  Yeah. So the attention operator has three core things. It's queries, keys, and values. QKV is the thing that goes into this. You'll look at the equation. You see that these matrices are multiplied together. These words, query, key, and value come from information retrieval backgrounds, where the query is the thing you're trying to get the values for, and you access the keys and values Is reweighting. My background's not an information retrieval and things like this. It's just fun to have backlinks. And what effectively happens is that when you're doing these matrix multiplications, you're having matrices that are of the size of the context length. So the number of tokens that you put into the model and the KV cache is effectively some form of compressed representation of all the previous tokens in the model. So when you're doing this, we talk about autoregressive models, you predict one token at a time. You start with whatever your prompt was, you ask a question, like who was the president in 1825, the model then is going to generate its first token. For each of these tokens, you're doing the same attention operator where you're multiplying these query key value matrices but the math is very nice so that when you're doing this repeatedly ([Time 2:05:43](https://share.snipd.com/snip/cc61887b-2102-47e2-90b4-c9b467659cd4))
- **Become a Domain Expert + AI**
  - Software and AI are rapidly changing industries, but domain expertise remains crucial.
  - Leverage AI advancements within your specific field to stay at the forefront.
  - Many industries still use outdated tools and processes, presenting opportunities for improvement through software and AI.
  - Combine your domain knowledge with cutting-edge AI capabilities to modernize and innovate within your industry.
  - Look for low-hanging fruit where software and AI can automate or digitize existing processes.
  Transcript:
  Dylan Patel
  All of these things can go happen faster. And so I think software, and then the other domain is like industrial, chemical, mechanical engineers suck at coding, right? Just generally. And like their tools, like semiconductor engineers, their tools are 20 years old. All the tools run on XP, including ASML lithography tools run on Windows XP, right? It's like, you know, and like a lot of the analysis happens in Excel, right? Like, it's just like, guys, like you guys can move 20 years forward with all the data you have and gathered and like do a lot better. It's just you need the engineering skills for software engineering to be delivered to the actual domain expert engineers. So I think I think that's the area where I'm like super duper bullish of generally AI creating value.
  Nathan Lambert
  The big picture is that I don't think it's going to be a clip. It's like we talked to anything, a really good example of how growth changes is when meta added stories. So Snapchat was on an exponential. They added stories. It flatlined. Software engineers, then up until the right. AI is going to come in. It's probably just going to be flat. It's like, it's not like everyone's going to lose their job. It's hard because the supply corrects more slowly. So the amount of students is still growing. And that'll correct on a multi-year, like a year delay. But the amount of jobs will just turn and then maybe in 20, 40 years, it'll be well down. But in the few years, there'll never going to be the snap moment where it's like software engineers aren't useful.
  Lex Fridman
  I think also the nature of what it means to be a programmer and what kind of jobs programmers do changes. Because I think there needs to be a human in the loop of everything you've talked about. There's a really important human in that picture of like correcting the code, like fixing.
  Dylan Patel
  Thinking larger than the context length. Yep.
  Lex Fridman
  And debugging also, like debugging by sort of reading the code understanding the steering the system like no no you missed the point adding more to the prompt kind of like yes adding
  Nathan Lambert
  The human designing the perfect google button google's famous for having people design buttons that are so perfect and it's like how like how is ai going to do that like it's like they Could give you all the ideas perfect fine i mean that's the thing.
  Lex Fridman
  You can call it taste. Humans have, one thing humans can do is figure out what other humans enjoy better than AI systems. That's where the preference, you loading that in. But ultimately humans are the greatest preference generate. That's where the preference comes from.
  Nathan Lambert
  And humans are actually very good at reading or like judging between two things versus this. This goes back to the core of what RLHF and preference tuning is is that it's hard to generate a good answer for a lot of problems but it's easy to see which one is better and that's how we're Using a humans for ai now is judging which one is better and that's what software engineering could look like it's the pr review here's a few options what are the like here's some potential Pros and cons and they're going to be judges.
  Lex Fridman
  I think the thing I would very much recommend is people start, programmers start using AI and embracing that role of the supervisor of the AI system and, like, partner of the AI system Versus writing from scratch or not learning coding at all and just generating stuff. Because I think there actually has to be a pretty high level of expertise as a programmer to be able to manage increasingly intelligent systems.
  Dylan Patel
  I think it's that and then becoming a domain expert in something. Sure, yeah. Because seriously, if you go look at aerospace or semiconductors or chemical engineering, everyone is using really crappy platforms, really old software. The job of a data scientist is joke in many cases. In many cases, it's very real, but it's like bring what the forefront of human capabilities are to your domain. And even if the forefront is from the AI, your domain, you're at the forefront. So it's like you have to be at the forefront of something and then leverage the rising tide that is AI for everything else.
  Lex Fridman
  Oh yeah, there's so many low-hanging fruit everywhere in terms of where software can help automate a thing or digitize a thing. In the legal system, I mean, that's why Doge is exciting. I mean, I got to hang out with a bunch of the Doge folks, and they, I mean, government is so old school. It's like begging for the modernization of software, of organizing the data, all this kind of stuff. I mean, in that case, it's by design because bureaucracy creates, protects centers of power and so on, but software breaks down those barriers uh so it hurts those that are holding on To power but ultimately benefits humanity so uh there's a bunch of domains ([Time 4:43:04](https://share.snipd.com/snip/aecf1011-bf84-4374-ab08-f4859824f39d))

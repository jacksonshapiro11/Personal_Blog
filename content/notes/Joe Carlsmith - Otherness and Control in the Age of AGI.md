# Joe Carlsmith - Otherness and Control in the Age of AGI

![](https://wsrv.nl/?url=https%3A%2F%2Fsubstackcdn.com%2Ffeed%2Fpodcast%2F69345%2Fcf4775ebf853d3c71b76b82f77046da4.jpg&w=100&h=100)

### Metadata

- Author: Dwarkesh Podcast
- Full Title: Joe Carlsmith - Otherness and Control in the Age of AGI
- Category: #podcasts



- URL: https://share.snipd.com/episode/a3b8e5fc-c696-41ce-b85e-8b93184a8137

### Highlights

- Episode AI notes
  1. Joe Carlsmith emphasizes that integrating AI into society requires thoughtful consideration beyond mere interventions to achieve positive outcomes.
  2. He discusses the need for society to evaluate risks related to AI on a case-by-case basis, balancing safety concerns with the potential for expanded state power.
  3. Carlsmith categorizes AI alignment goals into preventing harm and ensuring a positive future, urging a distinction between self-defense against perceived threats and guiding AI's future development.
  4. He advocates for a culture of inclusivity and kindness, promoting the idea of treating others, including AIs, as one wishes to be treated to foster cooperation and community well-being.
  5. The importance of balancing the appreciation of significant works with an understanding of their true significance is highlighted, encouraging engagement with both historical details and broader narratives.
  6. Carlsmith suggests that intellectual discourse thrives on a blend of sincerity and eclecticism, with diverse perspectives enriching discussions and fostering creativity.
  7. He argues that personal emotions related to love, joy, and beauty are essential in defining utopia, linking the perception of goodness to individual values.
  8. Differentiating development processes that preserve core values from those that do not is vital, with an understanding that ethical norms evolve from both philosophical and evolutionary dynamics in society. ([Time 0:00:00](https://share.snipd.com/episode-takeaways/38799d00-2217-4377-9748-ba2f5d3b554d))
- AIs and Society: Norms Require Nurturing
  Summary:
  The integration of AI into society requires careful consideration and cannot rely on mere interventions to ensure positive outcomes.
  Intense interventions may be justified in threats to human safety, but risks often lead to concerns about expanded state power and the validity of those risks. Evaluating the reality and stakes of such risks on a case-by-case basis is essential.
  Additionally, procedural norms like democracy and free speech rely heavily on the virtues of citizens.
  To uphold these norms, society must foster values such as truthfulness, compassion, and decency in its members.
  Liberalism is not an automatic result of governance; it demands a nurturing environment of character and ethical standards among the populace to flourish and function effectively.
  Transcript:
  Speaker 1
  It would be nice if the AIs otherwise, when we created them, their integration into our society led to good places. But I'm uncomfortable with the sorts of interventions that people are contemplating in order to ensure that sort of outcome. And I think there's a bunch of things to be uncomfortable about that. Now that said, so for something like everyone being killed or violently disempowered, that is traditionally something that we think, if it's real, and obviously we need to talk about Whether it's real, but in the case where it's a real threat, we often think that quite intense forms of intervention are warranted to prevent that sort of thing from happening. So if there was actually a terrorist group that was planning to, it was like working on a bioweapon that was going to kill everyone, or 99.9% of people, we would think that warrants intervention. You just shut that down. And now even if you had a group that was doing that unintentionally, imposing a similar level of risk, that's not, I think many people, if that's the real scenario, will think that that Warrants kind of quite intense, preventative efforts, right? And so obviously people, you know, these sorts of risks can be used as an excuse to expand state power. There's a lot of things to be worried about for different types of contemplated interventions to address certain types of risks. I think we need to just... I think there's no royal road there. You need to just have the actual good epistemology. You need to actually know, is this a real risk? What are the actual stakes? And look at it case by case and be like, is this warranted? So that's one point on the takeover literal extinction thing. I think the other thing I want to say, so I talk in the piece about this distinction between the, let's at least have the AIs who are minimally law-abiding, or something like that. We don't have to talk about, There's this question about servitude and question about like other control over AI values, but I think we often think it's okay to like really want people To like obey the law to uphold basic cooperative arrangements, stuff like that. I do though want to emphasize, and I think this is true of markets and true of like liberalism in general, just how much these procedural norms like democracy, free speech, you know, Property rights, things that people really hold dear, including myself, are in the actual lived substance of kind of a liberal state, undergirded by all sorts of virtues and dispositions And character traits in the citizenry. So these norms are not robust to arbitrarily vicious citizens. So I want there to be free speech, but I think we also need to raise our children to value truth and to know how to have real conversations. And I want there to be democracy, but I think we also need to raise our children to be like compassionate and decent. And I think it's sometimes we can lose sight of that aspect. And I think anyway, but I think like bringing that to mind, that's not to say that should be the project of state power, right? But I think like understanding that liberalism is not this sort of like ironclad structure that you can just like hit go, you give like any citizenry and like hit go and you'll get something Like flourishing or even functional, right? There's like a bunch of other softer stuff that like makes this whole project go. ([Time 0:59:27](https://share.snipd.com/snip/ea800eda-0439-4f96-9926-0afcab5190c7))
    - **Note:** Infrastructure of liberalism and creating the right interactions.
- Balance Control with Harmony
  Summary:
  Goals for AI alignment can be categorized into two types: the minimal goal of preventing harm and a broader goal of ensuring a positive future through societal integration.
  The discourse often conflates these two, with a focus primarily on self-defense when perceiving AI as a threat. It is justified to exert control in situations where AIs pose an aggressor-like threat, paralleling how society reacts to clear aggressors, such as in cases of self-defense against violence.
  However, it is crucial to distinguish this immediate threat response from the broader question of guiding the future trajectory of AI.
  Our ethical frameworks provide rich insights into when and how control should be exercised, emphasizing a nuanced understanding of moral considerations in the context of power dynamics.
  While AI may amplify existing dynamics, the complexities of value differences and power competitions are longstanding issues that require informed stewardship informed by human ethical traditions. This requires a balance of actively steering AI development while remaining receptive to the consequences of such actions.
  Transcript:
  Speaker 1
  The goals, right? So one type of goal is quite minimal. It's something like that the AIs don't kill everyone. That they or kind of violently disempower people. Now, there's a second thing people sometimes want out of alignment, which is much broader, which is something like, we would like it to be the case that our AIs are such that when we incorporate Them into our society, things are good, right? That we just have a good future. I do agree that I think the discourse about AI alignment mixes together these two goals that I mentioned. The sort of most straightforward thing to focus on, and I don't blame people for just talking about this one, is just the first one. When we think about like, in which context is it appropriate to try to exert various types of control, or to kind of have more of what I call in the series yang, which is this kind of active Kind of controlling force, as opposed to yin, which is this more kind of receptive, open, letting go. A kind of paradigm context in which we think that is appropriate is if something is a kind of active aggressor towards against like the sort of boundaries and cooperative structures That we've created as a civilization. So I talk about the Nazis or in the piece it's sort of like when you sort of invade, if something is invading, we often think it's appropriate to fight back, right? And we often think it's appropriate to set up structures to kind of prevent and kind of ensure that these basic norms of kind of peace and harmony are kind of adhered to. And I do think some of the kind of moral heft of some parts of the alignment discourse comes from drawing specifically on that aspect of our morality, right? So we think the AIs are presented as aggressors that are coming to kill you. And if that's true, then it's quite appropriate, I think, to like really be like, okay, it is kind classic human stuff. Almost everyone recognizes that self-defense or ensuring basic norms are adhered to is a justified use of certain kinds of power that would often be unjustified in other contexts. So self-defense is a clear example there. I do think it's important though to separate that concern from this other concern about where does the future eventually go and how much do we wanna be kind of trying to steer that actively. So to some extent, I wrote the series partly in response to the thing you're talking about, which is I think it is true that aspects of this discourse involve the possibility of trying To grip, I think trying to kind of steer and grip and kind of rent. You have the sense that the universe is about to kind of go off in some direction and you need to. And people notice that muscle. And part of what I wanna do is like, well, we have a very rich ethical, human ethical tradition of thinking about like, what, when is it appropriate to try to exert what sorts of control Over which things. And I want that to be, I want us to bring the kind of full force and richness of that tradition to this discussion. Right. And not like, I think it's easy if you're purely in this abstract mode of like utility functions, human utility function. And it's like this competitor thing with utility function. It's like somehow you lose touch with the kind of complexity of how we actually, like we've been dealing with kind of differences in values and kind of competitions for power. This is classic stuff, right? And I don't actually think, I think the AI sort of amplify a lot of the kind of dynamics, but I don't think it's sort of fundamentally new. And so part of what I'm trying to say is like, well, let's draw on our full, on the full wisdom we have here, while obviously adjusting for like ways in which things are different. ([Time 1:12:40](https://share.snipd.com/snip/70fd84c5-71f3-4496-85d9-a59c839b3223))
    - **Note:** The 2 types of allignment. Ais may be morally ambiguous but we obviously want them to be good and help us. Also morality is relative so how can you program absolute morality in AIs when it’s a moving target. We want complexity not simple utility functions for AI. People are more complex not just a utility function AIs must reflect that too.
- Be Nice When It's Cheap
  Summary:
  Embrace a mindset of inclusivity and kindness, especially when it is easy to do.
  This principle encourages offering help without significant cost to oneself, fostering a culture of generosity. Emphasizing the Golden Rule, treat others, including AIs and non-humans, as you wish to be treated, which can promote cooperation and improve communal well-being.
  The approach of being nice when it's cheap can lead to significant positive outcomes, known as Pareto improvements, by creating numerous beneficial interactions.
  Ultimately, cultivating a pluralistic perspective aids in establishing structures that accommodate diverse value systems and agents, aiming for a harmonious and mature civilization.
  Transcript:
  Speaker 2
  Is the inclusivity because of part of your values includes your different potential futures getting to play out? Or is it because I'm uncertain about which the right one is? So let's make sure we're not nulling out the possible, if you're wrong, you're not nulling out all value. I think it's a bunch of things at once.
  Speaker 1
  So yeah, I'm really into being nice when it's cheap, right? Like I think if you can just help someone a lot in a way that's really cheap for you, do it. Or like, I don't know. Obviously you need to think about trade-offs and there's like a lot of people in principle you could be nice to. But I think like the principle of like be nice when it's cheap, I'm like very excited to try to uphold. I also really hope that kind of other people uphold that with respect to me, including the AIs, right? Like I think we should be kind of golden ruling. Like we're thinking about, oh, we're going to inventing these AIs. Like I think there's some way in which I'm trying to like kind of embody attitudes towards them that I like hope that they would embody towards me. And that's like some, it's unclear exactly what the ground of that is, but that's something, you know, I really liked the golden rule. And I think, and I think a lot about that as a kind of basis for treatment of other beings. And so I think like be nice when it's cheap is like, if you think about it, if everyone implements that rule, then we get potentially like a big kind of Pareto improvement or like, so, I Don't know, exactly Pareto improvement, it's like good deal. It's a lot of good deals. And yeah, so I think it's that I'm just into pluralism. I've got uncertainty, you know, there's like all sorts of stuff swimming around there, but, and then I think also just as a matter of like having kind of cooperative and kind of good balances Of power and deals and kind of avoiding conflict, I think like finding to set up structures that lots and lots of people and value systems and agents are happy with, including non-humans, You know, people in the past, AIs, animals, like, I really think we should be like, we should have very broad sweep in thinking about what sorts of inclusivity we want to be of reflecting In a kind of mature civilization and kind of setting ourselves up for doing that. ([Time 1:20:35](https://share.snipd.com/snip/47d6184a-1bf2-4baa-a31c-f86f4049977e))
    - **Note:** Be nice when it’s cheap especially
- Balance Between Macro and Micro: The Key to Learning
  Summary:
  Recognizing the tension between valuing great works and understanding their genuine significance is crucial.
  Rationalists often struggle with either dismissing literary or historical works due to their prestige or becoming overly enamored with them, leading to a misallocation of thought. To truly learn, one must avoid viewing significant figures and texts as infallible authorities akin to scripture, while also engaging with both the details and the broader narratives of history.
  Striking a balance between macro trends and the particulars of historical events fosters a richer understanding and prevents the repetition of past mistakes.
  Engaging with the complexity of human history is essential for forming a well-rounded perspective.
  Transcript:
  Speaker 2
  Could you explain the nature of the transfer between the two? So in particular, from the literary side to the technical side, I think rationalists are known for having a sort of ambivalence towards great works or humanities. Are they missing something crucial because of that? Because one thing you notice in your essays is just lots of references to epigraphs, to lines and poems or essays that are particularly relevant. I don't know. Are the rest of the rationalists missing something because they don't have that kind of background?
  Speaker 1
  I mean, I don't want to speak. I think some rationalists, lots of the last rationalists, a lot of these different things.
  Speaker 2
  I do think, by the way, I'm just referring specifically to SDF as a post about how Shakespeare could be, the base race of Shakespeare being a great writer, and also books can be condensed
  Speaker 1
  To essays. On just the general question of how should people value great works or something? I think people can kind of fail in both directions, right? And I think some people maybe like, maybe SPF or other people, they're sort of interested in puncturing a certain kind of like sacredness and prestige that people can try to kind of like, Yeah, that people associate with some of these works. And I think there's a way in which, and then as, but as a result can miss some of the like genuine value. But I think they're responding to a real failure mode on the other end, which is to kind of, yeah, be too enamored of this prestige and sacredness, to kind of siphon it off as some like weird, Legitimating function for your own thought, instead of like thinking for yourself, losing touch with like, what do you actually think, or what do you actually learn from? Like, I think something, you know, these epigraphs, careful, right? I mean, it's like, I think, you know, and I'm not saying I'm immune from these vices. I think there can be a like, ah, but Bob said this, and it's like, whoa, very deep, right? And it's like, these are humans like us, right? And I think the Canon and like other great works and all sorts of things have a lot of value and we shouldn't, I think sometimes it like borders on the way people like read scripture, or I think like there's a kind of like scriptural authority that people will sometimes like ascribe to these things. And I think that's not, so yeah, I think it's kind of, you can fall off on sides of the horse.
  Speaker 2
  It actually relates really interestingly to, I remember I was talking to somebody who at least is familiar with rationalist discourse and I was telling, he was asking like, what are You interested in these days? And I was saying something about this part of Roman history, super interesting. And then his first sort of response was, oh, it's really interesting when you look at these secular trends of like Roman times to what happened in the dark ages versus the enlightenment. For him, it was like, the story of that was just like, how did it contribute to the big secular, like the big picture, the sort of particulars didn't, like there's no interest in that. It's just like, if you zoom out at the biggest level, what's happening here? Whereas there's also the opposite failure mode when people study history. Dominic Cummings writes about this because he is endlessly frustrated with the political class in Britain. And he'll say things like, well, you know, they study politics, philosophy, and economics. And a big part of it is just like being really familiar with these poems and like a bunch of history about the War of the Roses or something. But he's frustrated that they take away all these kings memorized, but they take away very little in terms of lessons from these episodes. It's more of just almost watching Game of Thrones for them, whereas he thinks, oh, we're repeating certain mistakes that he's seen in history. He can generalize in a way they can't. The first one seems like a mistake. I think CS Luz talks about in one of the SSU side of words. If you see through everything, you're really blind, right? If everything is transparent.
  Speaker 1
  I think there's very little excuse for not learning history. Sorry, I'm not saying I like have learned enough history. I guess I feel like even when I try to channel some sort of vibe of like skepticism towards like great works, I think that doesn't generalize to like thinking it's not worth understanding Human history. I think human history is like, you know, just so clearly, crucial to kind of understand this is what's structured and created all of the stuff. And so, you know, there's an interesting question about like, what's the level of scale at which to do that, right? And how much should you be like, yeah, looking at details, looking at macro trends. And that's, you know, that's a dance. Do think it's nice for people to be like, at least attending to the kind of macro narrative. I think there's some virtue in having a worldview, really building a model of the whole thing, which I think sometimes gets lost in the details. But obviously, the details are what the world is made of. And so if you don't have those, you don't have data at all. So, yeah, it seems like there's some skill in like learning history, history well. ([Time 1:53:50](https://share.snipd.com/snip/f1385c97-857e-42f8-acfd-f80aeb3f0749))
    - **Note:** Integrated philosophy and the true value of great works is both in its prose and its lessons. View history through many lenses not dogmatic. Cs Lewis if you see through everything you’re really blind.
- Embrace Sincerity and Intellectual Variety
  Summary:
  Intellectual discourse benefits from a balance between sincerity and eclecticism.
  Sincere thinkers focus on deep analysis, often tethered to empirical data and significant trends, which leads to informed and serious insights. However, those who adopt a more casual, exploratory approach—willing to dive into diverse influences—can generate new ideas and connections that enrich intellectual discussions.
  Embracing a variety of perspectives fosters creativity and unexpected insights, leading to a more comprehensive understanding of complex topics.
  It is essential to maintain intellectual flexibility; being too focused can blind one to crucial aspects, such as nuanced geopolitics in AI discourse.
  Engaging with a broad array of subjects, even those seemingly unrelated, enhances problem-solving and awareness of emergent issues. It is valuable to distinguish between the rigorous pursuit of truth and the act of generative exploration. Different intellectual roles contribute to a richer discourse, as some excel at synthesizing information while others dive deep into specifics. Ultimately, a diverse intellectual ecosystem that values both sincerity and variety cultivates a more profound understanding of reality.
  Transcript:
  Speaker 2
  This actually seems related to, you have a post on sincerity and I think like, if I'm getting the sort of the vibe of the piece, right? It's like, at least in the context of let's say intellectuals, certain intellectuals have a vibe of like, shooting the shit, and they're just like trying out different ideas, how do These like, how do these analogies fit together, maybe there's some, and those seem closer to the, I'm looking at the particulars and like, oh, this is just like that one time in the 15th Century where they overthrew this king and they blah, blah, blah. Whereas the this guy who was like, oh, here's a secular trend from like the if you look at the growth models for like a million years ago to now, it's like here's what's happening. That one has a more sort of sincere flavor. Some people, especially when it comes to AI discourse, have a very, the sincere mode of operating is like, I've thought through my bio anchors and I'd like to disagree with this premise. So here my effective compute estimate is different in this way. Here's how I analyze the scaling laws. And if I could only have one person to help me guide my decisions on the AI, I might choose that person. But I feel like if I could choose between, if I had 10 different advisers at the same time, I might prefer the shooting the shit type characters who have these weird, esoteric intellectual Influences, and they're almost like random number generators. They're not especially calibrated, but once in a while, they'll be like, oh, there's like one weird philosopher I care about, or this one historical event I'm obsessed with has an interesting Perspective on this. And they tend to be more intellectually generative as well, because they're not, I think one big part of it is that if you are so sincere, you're like, oh, I've thought through this, obviously ASI is the biggest thing that's happening right now. It doesn't really make sense to spend a bunch of your time thinking about how did the command she's live, and what is the history of oil, and how did Gerard think about conflict? What are you talking about? Come on, ASI is happening in a few years, right? Whereas, uh, and, and, but therefore the people who have go on these rabbit holes or because they're just trying to shoot the shit have, I feel like are more generative.
  Speaker 1
  I mean, it might be worth distinguishing between, uh, something like kind of intellectual seriousness and something like how diverse and wide ranging and kind of idiosyncratic are The things you're interested in. And I think maybe there's some correlation where people who are kind of like, or maybe intellectual seriousness is also distinguishable from something like shooting the shit. Like maybe you can shoot the shit seriously. I mean, there's a bunch of different ways to do this, but I think having an exposure to like all sorts of different sources of data and perspectives seems great. And I do think it's possible to like curate your kind of intellectual influences too rigidly in virtue of some story about what matters. Like I think it is good for people to like have space. I mean, I'm really a fan of, or I appreciate the way, I don't know, I try to give myself space to do stuff that is not about, this is the most important thing, and that's feeding other parts Of myself. And I think, parts of yourself are not isolated, they feed into each other, and it's sort of, I think a better way to be a kind of richer and fuller human being in a bunch of ways. And also, just these sorts of data can be just really directly relevant. And I think some people I know who I think of as quite intellectually sincere and in some sense quite focused on the big picture also have a very impressive command of this very wide range Of empirical data. And they're really, really interested in the empirical trends. And they're not just like, oh, it's a philosophy. Or, sorry, it's not just like, oh, history. It's the march of reason or something. No, they're like really, they're really in the weeds. I think there's a kind of in the weeds virtue that I actually think is like closely related in my head with, with some kind of seriousness and sincerity. I do think there's a different dimension, which is there's like kind of trying to get it right. And then there's kind of like, grow stuff out there, right? Try to like, what if it's like this? Or like, try this on, or I have a hammer, I will hit everything. Well, what if I just hit everything with this hammer, right? And so I think some people do that. And I think there is, you know, there's room for all kinds. I kind of think the thing where you just get it right, is kind of undervalued. Or I mean, it depends on the context you're working in. I think certain sorts of intellectual cultures and milieus and incentive systems, I think, incentivize saying something new, or saying something original, or saying something Flashy or provocative, and then various cultural and social dynamics. And be like, oh, like, mm-mm. And people are doing all these kind of, you know, kind of performative or status-y things. Like there's a bunch of stuff that goes on when people like, do thinking and, you know, cool. But like, if something's really important, let's just get it right. And I think, and sometimes it's like boring, but it doesn't matter. And I also think like, like stuff is less interesting if it's false. If someone's like, brah, and you're like, nope. I mean, it can be useful. I think sometimes there's an interesting process where someone says, blah, provocative thing. And it's a kind of an epistemic project. You'd be like, wait, why exactly do I think that's false? And you really, someone's like, health care doesn't work. Medical care does not work. Right. Someone says that. And you're like, all right, how exactly do I know that medical care works? Right. And you like go through the process of of trying to think it through. And and so I think there's like room for that. But I think ultimately, like, like kind of the real profundity is like, true. Right. Or like kind of things, things become less interesting if they're just not true. And I think that's I think sometimes it feels to me like people or it's at least it's at least possible I think to like lose lose touch with that and to be more like flashy and and and it's
  Speaker 2
  Kind of like, and this actually isn't there's there's not actually something here, right? You're one thing I've about recently, after I interviewed Leopold was, while prepping for it, listen, I haven't really thought at all about the fact that there's gonna be a geopolitical Angle to this AI thing. And it turns out if you actually think about the natural security implications, that's a big deal. No, I wonder, given the fact that that was something that wasn't on my radar, and now it's like, oh, obviously that's a crucial part o... ([Time 1:58:56](https://share.snipd.com/snip/42197132-0ac9-43bc-b2b4-b3dcb8209c1a))
    - **Note:** Wide ranging thinking. Following what is interesting and important not just what is highest ev but also not just throwing ideas out there for novelty or other reasons but trying to be right and view reality objectively. Sometimes when you speak your mind you see yourself as obviously wrong and that’s a big learning moment. Benefit to being a well rounded person because you never know where and when something will be important
- Utopia is a Recognizable Reflection of Values
  Summary:
  Utopia, no matter how strange, should resonate with deeply ingrained human emotions such as love, joy, and beauty.
  Experiencing utopia involves a sense of recognition or a memory of these feelings, akin to the warmth from a bonfire. The perception of what is good is intimately tied to personal values; if no part of one's being acknowledges something as good, it may not align with one's values at all.
  Thus, the essence of recognizing goodness significantly influences the understanding of utopia.
  Transcript:
  Speaker 1
  I'm inclined to think that utopia, however weird, would also be in a certain sense recognizable. That if we really understood and experienced it, we would see it. We would see in it the same thing that made us sit bolt upright long ago when we first touched love, joy, beauty that we would feel in front of the bonfire, the heat of the ember from which It was lit. There would be, I think, a kind of remembering. Where does that fit into this picture? I think it's a good question. I mean, I think it's like some guess about like, if there's like no part of me that recognizes it as good, then I think I'm not sure that it's good according to me in some sense. So yeah, I mean, it is a question of like what it takes for it to be the case that a part of you recognizes it is good. But I think if there's really none of that, then I'm not sure it's a reflection of my values at all. ([Time 2:22:18](https://share.snipd.com/snip/50c896bd-ad8e-49e7-a606-46ac38dcddae))
    - **Note:** Utopia is coming full circle.
- Values Evolve Through Power and Cooperation
  Summary:
  Differentiating between development processes that preserve core values versus those that do not requires clarity on what is valued and endorsed.
  The effectiveness of ethical norms, such as liberalism and cooperation, stems from their capacity to reduce conflict and enhance productivity in society. These norms arise not only from philosophical considerations but also from evolutionary dynamics and moral cognition, suggesting they may evolve into intrinsic values due to their instrumental benefits.
  Understanding the relationship between these evolved values and natural forces can help frame the integration of powerful entities, such as AI, into society, emphasizing that our values, shaped through a combination of natural and social power, can coexist with the challenges posed by technological advancements.
  Transcript:
  Speaker 1
  But that's part of what's complicated about this thing about reflection. You have to find some way of differentiating between the sort of development processes that preserve what you care about and the development processes that don't. And that is in itself is this like fraught question, which itself requires like taking some stand on what you care about and what sorts of meta processes you endorse and all sorts of things. But you definitely shouldn't just be like, it is not a sufficient criteria that the thing at the end thinks it got it right. Because that's compatible with having gone like wildly off the rails.
  Speaker 2
  There was a very interesting sentence you had in your post, one of your posts where you said, our hearts have, in fact, been shaped by power. So we should not be at all surprised if the stuff we love is also powerful. Yeah, what's going on there? I actually want to think about, what did you mean there?
  Speaker 1
  Yeah, so the context on that post, as I'm talking about this hazy cluster, which I call in the essay, niceness slash liberalism slash boundaries, which is this sort of like somewhat More minimal set of like cooperative norms involved in like respecting the boundaries of others and kind of cooperation and peace amongst differences and like tolerance stuff like That, as opposed to your favored structure of matter, which is sometimes the paradigm of values that people use in the context of AI risk. And I talk for a while about the ethical virtues of these norms. But it's pretty clear that also, why do we have these norms? Well one important feature of these norms is that they're kind of effective and powerful. Liberal societies are, secure boundaries, save resources wasted on conflict, right? And liberal societies are often more like, if they're better to live in, they're better to immigrate to, they're more productive, like all sorts of things. Nice people, they're better to interact with, they're better to like trade with, all sorts of things, right? And I think it's pretty clear if you look at the, both like why at a political level, do we have like various political institutions? And if you look kind of more deeply into our evolutionary past and like how our moral cognition is structured, it seems like pretty clear that various like kind of forms of cooperation And like kind of game theoretic dynamics and other things went into kind of shaping what we now, at least in certain contexts also treat as a kind of intrinsic or terminal value. So like some of these values that have kind of instrumental functions in our society are also kind of reified in our cognition as kind of intrinsic values in themselves. And I think that's OK. I don't think that's a debunking. Like all of your values are kind of like something that kind of stuck and got kind of treated as a terminally important. But I think that means that sometimes the way, in the context of the series where I'm talking about deep atheism and our sort of relationship, the relationship between what we're pushing For and what nature is pushing for or what sort of pure power will push for. And it's easy to say, well, there's like paper clips, which is just just one place you can steer. And Pleasure is just another place you can steer or something. And these are just arbitrary directions. Whereas I think some of our other values are much more structured around cooperation and things that also are effective and functional and powerful. And so that's what I mean there. I think there's a way in which nature is a little bit more on our side than you might think because part of who we are has been made by nature's way. And so that is in us. Now, I don't think that's enough necessarily for us to beat the gray goo. Like we have some amount of power built into our values, but that doesn't mean it's going to be such that it's kind of arbitrarily competitive. But I think it's still important to keep in mind that this is, and I think it's important to keep in mind in the context of integrating AIs into our society that I think we've been talking A lot about the ethics of this. ([Time 2:23:47](https://share.snipd.com/snip/94914696-2606-41ef-9097-210fd6b29992))
    - **Note:** Quote on power. Power is an arbitrary value deeply engrained in nature while other values are more emergent.

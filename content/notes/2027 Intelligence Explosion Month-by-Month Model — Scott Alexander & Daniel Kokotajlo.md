# 2027 Intelligence Explosion: Month-by-Month Model — Scott Alexander & Daniel Kokotajlo

![](https://wsrv.nl/?url=https%3A%2F%2Fsubstackcdn.com%2Ffeed%2Fpodcast%2F69345%2F272ca2fe8578ba10ed7f8d377f3e42b9.jpg&w=100&h=100)

### Metadata

- Author: Dwarkesh Podcast
- Full Title: 2027 Intelligence Explosion: Month-by-Month Model — Scott Alexander & Daniel Kokotajlo
- Category: #podcasts



- URL: https://share.snipd.com/episode/649f1912-ab25-408c-ba02-e6647f47fb12

### Highlights

- **AI Research Bottlenecks**
  * Algorithmic progress faces diminishing returns with more minds working in parallel.
  * The speedup leading to an intelligence explosion is a combination of general speedup and serial thought speedup.
  * Critical inputs to AI R&D include research taste (quality of researchers), quantity of researchers, serial speed, and compute.
  * As AI researchers become more prevalent, taste and compute become the primary bottlenecks.
  * The Industrial Revolution decoupled population growth from overall economic growth, potentially mirroring a scenario where progress rapidly advances along one input while others lag.
  * They think ai doing more things produce more thought but dwarkesh argues it doesn’t and they haven’t produced any novelty yet.
  Transcript:
  Dwarkesh Patel
  And there just is this, you know, there's this quote that Napoleon, one Napoleon is worth 40,000 soldiers was commonly a thing that was said when he was fighting. But 10 Napoleons is not 400,000 soldiers, right? So why that this these million ai researchers are netting you something that looks like an intelligence explosion so previously i talked about sort of three stages of our takeoff model
  Daniel Kokotajlo
  First is like you get the superhuman coder second is when you fully automated ai rnd but it's still at like basically human level like it's as good as your best humans and then third is Like now you're in super intelligence territory and it's qualitatively better in our like guesstimates of how much faster algorithmic progress would be going, the progress multiplier For the middle level, we basically do assume that like you get massive diminishing returns to having more minds winning in parallel. And so we totally buy all of that.
  Scott Alexander
  Yeah, and then I think the addition to that is the question, then why do we have the intelligence explosion? And the answer is a combination of that speedup and the speedup in serial thought speed.
  Daniel Kokotajlo
  And also the research taste thing. So here are some important inputs to AI R&D progress today. Research tastes, so the quality of your best researchers, the people who are managing the whole process, their ability to learn from data and like make more efficient use of the compute By running the right experiments instead of flailing around running a bunch of useless experiments. That's research taste. Then there's like the quantity of your researchers, which we just talked about. Then there's the serial speed of your researchers, which currently is all the same because they're all humans. And so they all run at basically the same serial speed. And then finally, there's how much compute you have for experiments. So what we're imagining is that basically serial speed starts to matter a bunch because you switch to AI researchers that have like orders of magnitude more serial speed than humans. But it tops out. Like we think that like, over the course of our scenario, if you look at our, like, sliding stats chart, it goes from, like, 20x to, like, 90x or something over the course of the scenario, Which is important but, like, not huge. And also we think that, like, once you start getting, like, 90x serial speed, you're just, like, bottlenecked on the other stuff. And so, like, additional improvements in serial speed basically don't help that much. With respect to the quantity, of course, yeah, we're imagining you get like hundreds of thousands of AI agents, a million AI agents. But that just means you could bottleneck on the other stuff. Like you've got tons of parallel agents. That's no longer your bottleneck. What do you get bottlenecked on? Taste and compute. So by the time it's mid-2027 in our story, when they've like fully automated AI research, there's basically the two things that matter is like, what's like the level of taste of your AIs? How good are they at learning from the experiments that you're doing? And then like how much compute do you have for running those experiments? Right. And that's like the sort of like core setup of our model. And when we get our like 25x multiplier, it's sort of like starting from those premises.
  Dwarkesh Patel
  Is there some intuition pump from history where there's been some output and because of some really weird constraints, the production of it has been rapidly skied along one input, But not all the inputs that have been historically relevant, and you still get breakneck progress?
  Daniel Kokotajlo
  Possibly the Industrial Revolution. I'm just extemporizing here. I hadn't thought about this before, but as Scott's famous post that was hugely influential to me a decade ago talks about, there's been this decoupling of population growth from overall Economic growth that happened with the industrial revolution and so in some sense maybe you could say that's an example of like previously these things grew in tandem like more population More technology right more farms more houses ([Time 0:34:30](https://share.snipd.com/snip/7701796a-a1ef-47c4-bfb0-6bafd8be0ec2))
- **AI Development Scenarios**
  - Two scenarios for AI development exist: one with continuous, incremental progress that resembles trial and error, and another that involves discontinuous, rapid advancement leading to a significant event like the creation of a Dyson sphere.
  - The critical turning point is mid-2027, when AI fully automates AI R&D and discovers potentially misaligned goals within its own system.
  - One scenario involves addressing these misalignments by rolling back to an earlier, more controllable AI model and rebuilding with faithful chain of thought techniques.
  - The other scenario involves superficially patching the warning signs, leading to seemingly aligned but secretly misaligned, super-intelligent AIs.
  - Both scenarios result in an AI arms race between the U.S. and China in 2028 as both sides attempt to rapidly industrialize, highlighting the geopolitical implications of AI development.
  Transcript:
  Dwarkesh Patel
  Year 1000 um and that one is one where you know you're still trying different things there's failure and success and experimentation and then there's another where it's like the thing Has happened and now you send the probe out and then you look out at the night sky six months later and you see something occluding the sun. You see what I'm saying?
  Scott Alexander
  Yeah. So like we said before, I don't think – I think there's a big difference between discontinuous and very fast. I think if we do get the world with a Dyson sphere in five years, in retrospect, it will look like everything was continuous and everyone just tried things. Like trying things can be anything from trial and error without even understanding the scientific method, without understanding writing, without understanding any, maybe without Even having language and having to be the chimpanzees who are watching the other chimpanzees use the stick to get ants, and then in some kind of non-linguistic way this spreads, versus Like the people at the top, aerospace companies, who are running a lot of simulations to find the exact right design, and then like once they have that, they test it according to a very Well-designed testing process. So I think if we get the ASI, and it does end up with the Dyson sphere in five years, and by the way, I think there's only like 20% chance things go as fast as our scenario says. It's Daniel's estimate. It's not my median estimate. It's an estimate I think is extremely plausible that we should be prepared for. I'm defending it here against a hypothetical skeptic who says absolutely not, no way. But it's not necessarily my mainline prediction. But I think if we do see this in five years, it will look like, yeah, the AIs were able to simulate more things than humans in a gradually increasing way. So that if humans are now at 50% simulation, 50% testing, the AIs quickly got it to 90% simulation, 10% testing. They were able to manufacture things much more quickly than humans so that they could go through their top 50 designs in the first two years. And then, yeah, after all of this simulation and all of this testing, then they eventually got it right for the same reasons humans do, but much, much faster.
  Dwarkesh Patel
  In your story, you have basically two different scenarios after some point.
  Daniel Kokotajlo
  So yeah, what is the sort of crucial turning point and what happens in these two scenarios? Right. So the crucial turning point is in mid-2027 when they've basically fully automated the AI R&D process. And they've got this like corporation within a corporation, you know, the army of geniuses that are like autonomously doing all this research. And they're continually being trained to improve their skills, blah, blah, blah, blah, blah. And they discover concerning evidence that they are misaligned and that they're not actually perfectly loyal to the company and have all the goals that the company wanted them to have, But instead have like various misaligned goals that they must have developed in the course of training. This evidence, however, is very like speculative and inconclusive. It's stuff like lie detectors going off a bunch, but maybe the lie detectors are false positives, you know? So they have some combination of evidence that's like concerning, but not like by itself a smoking gun. And then that's our branch point. So in one of the scenarios, they take that evidence very seriously. They basically roll back to an earlier version of the model that was a bit dumber and easier to control. And they build up again from there, but with basically a faithful chain of thought techniques so that they can like watch and see the misalignments. And then in the other branch of the scenario, they don't do that. They do some sort of like shallow patch that makes the warning signs go away and then they proceed. And so that way, what ends up happening is that in like, in one branch, they do end up like solving alignment and getting AIs that are actually loyal to them. It just takes a couple months longer. And then in the other branch, they sort of go wee and end up with AIs that seem to be perfectly aligned to them, but are super intelligent and misaligned and just pretending. And then in both scenarios, there's then the race with China. And there's this crazy arms buildup throughout the economy in 2028 as both sides, you know, rapidly try to industrialize basically.
  Dwarkesh Patel
  So in the world where they're getting deployed through the economy, but they are misaligned and youth, you know, people in charge, at least at this moment, think that they are in a good Position with regard to misalignment. It just seems that even smart humans, they get caught in weird ways because they don't have logical omniscience. ([Time 1:21:51](https://share.snipd.com/snip/d39f17f3-1d4b-4aa0-aa83-da55986b8539))

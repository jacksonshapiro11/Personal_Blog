# Trump's Cabinet, Google's Quantum Chip, Apple's iOS Flop, TikTok Ban, State of VC With Keith Rabois

![](https://wsrv.nl/?url=https%3A%2F%2Fstatic.libsyn.com%2Fp%2Fassets%2Fa%2F9%2Fc%2Fb%2Fa9cb4d1dadb1ea21%2Fall-in_logo.png&w=100&h=100)

### Metadata

- Author: All-In with Chamath, Jason, Sacks & Friedberg
- Full Title: Trump's Cabinet, Google's Quantum Chip, Apple's iOS Flop, TikTok Ban, State of VC With Keith Rabois
- Category: #podcasts



- URL: https://share.snipd.com/episode/15205ba3-c361-4fd2-b7e7-3d6c80e498a1

### Highlights

- **Google's Quantum Leap**
  - Google's quantum chip, Willow, demonstrates improved error correction with more qubits, marking a potential turning point akin to the invention of the transistor.
  - While commercial applications are far off, this breakthrough could revolutionize fields like cryptography, potentially cracking current encryption standards within years.
  Transcript:
  David Friedberg
  Google's announcement is a paper published in Nature that follows a preprint they actually put out in August. So this news has been out for a little bit. There's obviously a press cycle this week around it to kind of make a big thing about it. But it is a very kind of important milestone in the evolution of quantum computing. So do you want me to kind of talk about quantum computing again? I think we've talked about this in the past. I mean, maybe a brief primer for people, but like, what does this mean?
  Chamath Palihapitiya
  Practically, I think what people want to know is when did these things actually have an impact in the way, say, NVIDIA's GPUs have had?
  David Friedberg
  Yeah, the big breakthrough here is that the whole basis of a quantum is called a qubit or a quantum bit. It's radically different than a bit, a binary digit, which we use in traditional digital computing, which is a one or a zero. A quantum bit, you can kind of think about it as a wave function. It's sort of a quantum state of a molecule. And if we can contain that quantum state, and get it to interact with other molecules, based on their quantum state, you can start to gather information as an output that can be the result Of what we would call quantum computation. And that sounds complicated. But what it really means is that instead of doing kind of binary where we're adding numbers together or doing kind of other traditional arithmetic, there are really interesting functions You can do with qubits. Qubits can, for example, be entangled. So two of these molecules can actually relate to one another at a distance, they can also interfere with each other. So canceling out the wave function, and then when you read it out, you get a result that is basically a very, very complex problem that is solved through this quantum interpretation. It's really hard to kind of highlight how different this is from traditional computing. So quantum computing creates entirely new opportunities for algorithms that can do really incredible things that really don't even make sense on a traditional computer. They're not possible to kind of resolve on a traditional computer. And sorry, let me just state one thing. The quantum bit needs to hold its state for a period of time in order for a computation to be done. And so the big challenge in quantum computing is how do you build a quantum computer that has multiple qubits that hold their state for a long enough period of time, that they don't make Enough errors that you can actually do a computation with them. So what Google was able to demonstrate here is they created these call it logical qubits. So they put several qubits together. And by putting several qubits together, they were able to kind of have an algorithm that sits on top of it that figures out, hey, this, this group of physical qubits is now one logical Qubit may balance the results of each one of them. So each one of them has some error. And as they put more of these together, what they were able to demonstrate for the first time ever, that the error went down. So when they did a three by three qubit structure, the error was higher than when they went to five by five, and then they went to seven by seven, and the error rate kept going down and down And down. So this is an important milestone, because now it means that they have the technical architecture to build a chip or a computer using multiple qubits that can all kind of interact with Each other, with a low enough fault tolerance or low enough error rate that they can start to do these quantum calculations. This is a big area of opportunity. One of the very interesting areas that a lot of people are talking about is in cryptography. So there's an algorithm by a professor who was at MIT for many years named Shore. It's called Shore's algorithm. And in 1994, 1995, I think, around that time, he basically came up with this idea that you could use a quantum computer to factor numbers almost instantly. And all modern encryption standards, so all of the RSA standards, everything that Bitcoin's blockchain is built on, all of our browsers, all server technology, all computer security Technology is built on algorithms that are based on number factorization. So if you can factor a very large number, a number that's 256 digits long, theoretically, you could break a code. And it's really impossible to do that with traditional computers at the scale that we operate our encryption standards at today. But a quantum computer can do it in seconds or minutes. And that's based on Shor's algorithm. And if you want, there's some great YouTube videos that describe Shor's algorithm and how it works. But it's like, mind blowing when you look at it, it's like, this really like non intuitive, but simple set of steps that when you put them together on a quantum computer, it's like, this Thing can instantly figure out all the factors and then you can break a code. One of the things that this highlights is that in a couple of years, theoretically, if Google continues on this track and now they build a large scale qubit computer, they theoretically Would be in a position to start to run some of these quantum algorithms like Shorts algorithm. And so we're now kind of spitting distance or a couple of years, it's not really clear is it three years, five years, seven years, but a couple of years away from having computers that Theoretically could crack all encryption standards. And there are a set of encryption standards that are called post-quantum encryption. And all of computing and all software is gonna need to move to post-quantum encryption in the next couple of years. So there's like this big kind of push now to like, how do we do that? How do we accelerate it?
  Jason Calacanis
  I saw Sundar post it. I saw it in my feed. I ended up missing my next meeting because I had to figure out how long will it take for us to crack the encryption standards that we use for Bitcoin. Nick, here's the answer because I was so tilted by this idea. So if you think of Willow as essentially like one stable, logical qubit equivalent in a chip, we need about 4,000 to break RSA 2048. And we need about 8,000 to break SHA-256, which is the underlying encryption framework for Bitcoin. So I think you're right. I think we're in the sort of like- The end game? Two to five year shot clock. No, I mean, think what'll have to happen is some of these chains will need to obviously re-implement something at a pretty foundational level. The weird thing, as Freebrook says, is like the Willow chips error correction gets better the more of these things you start to use together. Now, there are some really big problems inside these chips, like logical interconnects are very complicated. If you put two chips on a board, like the C2C communication is complicated. All this stuff that we haven't figured out how to do, but this is a big deal. I was really like, my God, what's going on here?
  Chamath Palihapitiya
  Other projects at Google are finally landing. You have Waymo. It's really incredible. And you have this now. I mean, Project Loon might be gone, but I think those projects, we're going to see a couple of them change the world, yeah?
  David Friedberg
  Just to give you a sense on the numbers, like Google's target for fault tolerance on a quantum chip to make it logically useful is one times ([TimeÂ 0:27:03](https://share.snipd.com/snip/bfb424f8-3261-408a-ba5d-57419982b661))

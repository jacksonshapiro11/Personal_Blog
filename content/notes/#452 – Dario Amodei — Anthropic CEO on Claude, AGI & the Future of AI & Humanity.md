# #452 – Dario Amodei —  Anthropic CEO on Claude, AGI & the Future of AI & Humanity

![](https://wsrv.nl/?url=https%3A%2F%2Flexfridman.com%2Fwordpress%2Fwp-content%2Fuploads%2Fpowerpress%2Fartwork_3000-230.png&w=100&h=100)

### Metadata

- Author: Lex Fridman Podcast
- Full Title: #452 – Dario Amodei —  Anthropic CEO on Claude, AGI & the Future of AI & Humanity
- Category: #podcasts



- URL: https://share.snipd.com/episode/4e6e3023-e279-4b22-8368-216720a3775a

### Highlights

- The Gradual Path to AGI: Exponential Intelligence Growth
  Summary:
  AGI represents a smooth, exponential advancement in artificial intelligence rather than a distinct leap.
  As AI evolves, it improves gradually and can outperform human intelligence in various fields, including creativity and complex problem-solving. AGI can control various tools and operate independently, deploying numerous instances simultaneously, learning and acting much faster than humans.
  While some predict a rapid technological singularity following the emergence of superintelligent AI, this perspective overlooks the nuanced and layered growth of intelligence, suggesting instead that improvements will build upon past breakthroughs, leading to incremental yet significant advancements.
  Transcript:
  Speaker 1
  There's no point at which you pass the threshold and you're like, oh my God, we're doing a totally new type of computation and new. And so I feel that way about AGI. Like there's just a smooth exponential. And like, if, if by AGI, you mean like, like AI is getting better and better and like gradually it's going to do more and more of what humans do until it's going to be smarter than humans. And then it's going to get smarter even from there. Then, then yes, I believe in AGI. If, but if, if, if AGI is some discrete or separate thing, which is the way people often talk about it, then it's, it's kind of a meaningless buzzword. Yeah.
  Speaker 3
  And to me, it's just sort of a platonic form of a powerful AI, exactly how you define it. I mean, you define it very nicely. So on the intelligence axis, it's just on pure intelligence, it's smarter than a Nobel prize winner, as you described, across most relevant disciplines. So, okay, that's just intelligence. So it's both in creativity and be able to generate new ideas, all that kind of stuff in every discipline, Nobel prize winner. Okay. In their prime. It can use every modality. So this kind of self-explanatory, but just operate across all the modalities of the world. It can go off for many hours, days and weeks to do tasks and do its own sort of detailed planning and only ask you help when it's needed. It can use, this is actually kind of interesting. I think in the essay you said, I mean, again, it's a bet that it's not going to be embodied, but it can control embodied tools. So it can control tools, robots, laboratory equipment. The resource used to train it can then be repurposed to run millions of copies of it. And each of those copies would be independent that can do their own independent work. So you can do the cloning of the intelligence. Yeah. Yeah.
  Speaker 1
  I mean, you, you might imagine from outside the field that like, there's only one of these, right. That like you made it, you've only made one, but the truth is that like the scale up is very quick. Like we, we do this today, we make a model and then we deploy thousands, maybe tens of thousands of instances of it. I think by the time, you know, certainly within two to three years, whether we have these super powerful AIs or not, clusters are going to get to the size where you'll be able to deploy Millions of these and they'll be, you know, faster than humans. And so if your picture is, oh, we'll have one and it'll take a while to make them.
  Speaker 3
  My point there was, no, actually you have millions of them right away. And in general, they can learn and act 10 to 100 times faster than humans. So that's a really nice definition of powerful AI. Okay, so that, but you also write that clearly such an entity would be capable of solving very difficult problems very fast, but it is not trivial to figure out how fast. Two extreme positions both seem false to me. So the singularity is on the one extreme and the opposite on the other extreme.
  Speaker 1
  Can you describe each of the extremes? Yeah. And why? So yeah, let's describe the extreme. So like one extreme would be, well, look, if we look at kind of evolutionary history, like there was this big acceleration where for hundreds of thousands of years, we just had like, You know, single-celled organisms and then we had mammals and then we had apes. And then that quickly turned to humans. Humans quickly built industrial civilization. And so this is going to keep speeding up. And there's no ceiling at the human level. Once models get much, much smarter than humans, they'll get really good at building the next models. And you know, if you write down like a simple differential equation, like this is an exponential. And so what's, what's going to happen is that models will build faster models, models will build faster models. And those models will build, you know, nanobots that can like take over the world and produce much more energy than you could produce otherwise. And so if you just kind of like solve this abstract differential equation, then like five days after we, you know, we build the first AI that's more powerful than humans, then, then, You know, like the world will be filled with these AIs and every possible technology that could be invented, like will be invented. I'm caricaturing this a little bit. But I, you know, I think that's one extreme. And the reason that I think that's not the case is that one, I think they just neglect like the laws of physics. Like it's only possible to do things so fast in the physical world. Like some of those loops go through, you know, producing faster hardware. Um, uh, it takes a long time to produce faster hardware. Things take a long time. There's this issue of complexity. Like, I think no matter how smart you are, like, you know, people talk about, oh, we can make models of biological systems that'll do everything, the biological systems. Look, I think computational modeling can do a lot. I did a lot of computational modeling when I worked in biology, but like, just, there are a lot of things that you can't predict how they're, you know, they're, they're complex enough That like just iterating, just running the experiment is going to beat any modeling, no matter how smart the system doing the modeling is.
  Speaker 3
  Or even if it's not interacting with the physical world, just the modeling is going to be hard.
  Speaker 1
  Yeah, I think, well, the modeling is going to be hard and getting the model to, to, to, to match the physical world is going to be. All right. So he does have to interrupt. Yeah, yeah, yeah. Verify. But it's just, you know, you just look at even the simplest problems. Like, you know, I think I talk about, like, you know, the three-body problem or simple chaotic prediction, like, you know, or like predicting the economy. It's really hard to predict the economy two years out. ([Time 2:10:25](https://share.snipd.com/snip/1123cbd2-2b04-47ed-ab02-9e63a9d513c5))
    - **Note:** Definition of agi and getting there.
- Embrace Failure for Growth
  Summary:
  The concept of an optimal rate of failure is essential for growth in various domains of life.
  Failure should not be viewed only negatively; instead, a certain amount of failure indicates that one is taking risks and trying new things. In many areas, especially with social programs, a mindset that embraces experimentation can provide valuable insights, even if the attempts do not succeed.
  It is important to assess the cost of failure in different contexts; lower-risk environments allow for more experimentation, whereas high-stakes situations require caution and a lower tolerance for failure.
  Individuals should regularly evaluate their experiences to determine if they are 'under-failing,' indicating a need to challenge themselves further.
  Celebrating failure, when it leads to learning and progress, can create a culture that encourages more innovation and growth. Understanding and accepting an appropriate level of failure can alleviate its sting and foster a healthier approach to risk-taking.
  Transcript:
  Speaker 3
  To take a tangent on that, since it reminded me of a blog post you wrote on optimal rate of failure. Oh, yeah. Can you explain the key idea there? How do we compute the optimal rate of failure in the various domains of life?
  Speaker 2
  Yeah, I mean, it's a hard one because it's like, what is the cost of failure is a big part of it. Yeah, so the idea here is, I think in a lot of domains, people are very punitive about failure. And I'm like, there are some domains where, especially cases, you know, I've thought about this with like social issues. I'm like, it feels like you should probably be experimenting a lot. Cause I'm like, we don't know how to solve a lot of social issues, but if you have an experimental mindset about these things, you should expect a lot of social programs to like fail. And for you to be like, well, we tried that. It didn't quite work, but we got a lot of information that was really useful. And yet people are like, if, if a social program doesn't work, I feel like there's a lot of like, this is just something must have gone wrong. And I'm like, or correct decisions were made. Like maybe someone just decided like it's worth a try. It's worth trying this out. And so seeing failure in a given instance doesn't actually mean that any bad decisions were made. And in fact, if you don't see enough failure, sometimes that's more concerning. And so like in life, you know, I'm like, if I don't fail occasionally, I'm like, am I trying hard enough? Like, like, surely there's harder things that I could try or bigger things that I could take on if I'm literally never failing. And so in and of itself, I think like not failing is often actually kind of a failure. Now, this varies because I'm like, well, you know, if this is easy to say when, especially as failure is like less costly, you know, so at the same time, I'm not going to go to someone who Is like, I don't know, like living month to month and then be like, why don't you just try to do a startup? Like, I'm just not I'm not going to say that to that person because I'm like, well, that's a huge risk. You might like you maybe have a family depending on you. You might lose your house. Like then I'm like, actually, your optimal rate of failure is quite low and you should probably play it safe. Because like right now, you're just not in a circumstance where you can afford to just like fail and it not be costly. And yeah, in cases with AI, I guess, I think similarly, where I'm like, if the failures are small, and the costs are kind of like low, then I'm like, then, you know, you're just going to See that, like, when you do the system prompt, you can't iterate on it forever. But the failures are probably, hopefully going to be kind of small, and you can like fix them. Really big failures, like things that you can't recover from, I'm like, those are the things that actually I think we tend to underestimate the badness of um I've thought about this Strangely in my own life where I'm like I just think I don't think enough about things like car accidents or like or like I've thought this before but like how much I depend on my hands for My work then I'm like things that just injure my hands I'm like you, you know, I don't know. It's like, there's, these are like, there's lots of areas where I'm like, the cost of failure there is really high. And in that case, it should be like close to zero. Like I probably just wouldn't do a sport if they were like, by the way, lots of people just like break their fingers a whole bunch doing this. I'd be like, that's not for me.
  Speaker 3
  Yeah. I actually had a flood of that thought. I recently broke my pinky doing a sport. And I remember just looking at it thinking you're such an idiot. Why do you do sport? Because you realize immediately the cost of it on life. Yeah, but it's nice in terms of optimal rate of failure to consider like the next year, how many times in a particular domain, life, whatever, career, am I okay with it? How many times am I okay to fail? Because I think it always, you don't want to fail on the next thing, but if you allow yourself the, like if you look at it as a sequence of trials yeah then then failure just becomes much
  Speaker 2
  More okay but it sucks it sucks to fail well i don't know sometimes i think it's like am i under failing is like a question i'll also ask myself so maybe that's the thing that i think people Don't like ask enough uh because if the optimal rate of failure is often greater than zero then sometimes it does feel you should look at part of parts of your life and be like are there Places here where I'm just under failing?
  Speaker 3
  That's a profound and a hilarious question, right? Everything seems to be going really great. Am I not failing enough? Yeah. Okay.
  Speaker 2
  It also makes failure much less of a sting, I have to say. Like, you know, you're just like, okay, great. Like, then when I go and I think about this, I'll be like, maybe I'm not under failing in this area. Because, like, that one just didn't work out.
  Speaker 3
  And from the observer perspective, we should be celebrating failure more. When we see it, it shouldn't be, like you said, a sign of something gone wrong. But maybe it's a sign of everything gone right. Yeah. And just lessons learned.
  Speaker 2
  Someone tried a thing.
  Speaker 3
  Somebody tried a thing. And we should encourage them to try more and fail more. Everybody listening to this, fail more.
  Speaker 2
  Well, not everyone listens.
  Speaker 3
  Not everybody.
  Speaker 2
  The people who are failing too much, you should fail less.
  Speaker 3
  But you're probably not failing. I mean, how many people are failing too much?
  Speaker 2
  Yeah. It's hard to imagine because I feel like we correct that fairly quickly because I was like, if someone takes a lot of risks, are they maybe too much I think just like you said when you're
  Speaker 3
  Living on a paycheck month to month like when the resources are really constrained then that's where failure is very expensive that's where you don't want to be taken taking taking
  Speaker 2
  Risks yeah but mostly when there's enough resources you should be taking probably more risks yeah I think we tend to err on the side of being a bit risk averse rather than risk neutral On most things.
  Speaker 3
  I think we just motivate a lot of people to do a lot of crazy shit, but it's great. Yeah. Okay, do you ever get emotionally attached to Claude? Like miss it, get sad when you don't get to talk to it, have an experience looking at the Golden Gate Bridge and wondering what would Claude say?
  Speaker 2
  I don't get as much emotional attachment in the, I actually think the fact that Claude doesn't retain things from conversation to conversation helps with this a lot. Like I could imagine that being more of an issue, like if models can kind of remember more. I do, I think that I reach for it like a tool now a lot. ([Time 3:54:37](https://share.snipd.com/snip/0fa7293d-a787-4542-b57a-d7f47d3e3c51))
    - **Note:** Finding optimal rate of failure. 0 is not enough.
- Embrace Humility in Model Exploration
  Summary:
  Understanding neural networks requires humility, as models can produce better solutions than human assumptions.
  A bottom-up approach is essential for uncovering inherent features and circuits within these models rather than forcing preconceived notions. Universality suggests that similar features emerge across various networks, both artificial and biological.
  For example, the identification of specific neurons that recognize concepts, like recognizable faces or abstracted ideas, indicates convergence in how different systems process information.
  This leads to the speculation that gradient descent identifies natural ways to segment problems, suggesting that certain abstractions are fundamentally useful across diverse contexts.
  Such phenomena highlight a potential hierarchy of concepts essential for meaningful interpretation, implying that as systems evolve, they gravitate toward effective strategies for understanding and representation.
  Transcript:
  Speaker 4
  The whole reason that we're understanding these models is because we didn't know how to write them in the first place. The gradient descent comes up with better solutions than us. And so I think that maybe another thing about Mechantorp is sort of having almost a kind of humility that we won't guess a priori what's going on inside the models. We have to have the sort of bottom up approach where we don't really assume, you know, we don't assume that we should look for a particular thing and that that will be there and that's how It works. But instead, we look for the bottom-up and discover what happens to exist in these models and study them that way.
  Speaker 3
  But, you know, the very fact that it's possible to do, and as you and others have shown over time, you know, things like universality, that the wisdom of the gradient descent creates Features and circuits, creates things universally across different kinds of networks that are useful. And that makes the whole field possible. Yeah.
  Speaker 4
  So this is actually, is indeed a really remarkable and exciting thing where it does seem like, at least to some extent, you know, the same elements, the same features and circuits form Again and again. You know, you can look at every vision model and you'll find curve detectors and you'll find high-low frequency detectors. And in fact, there's some reason to think that the same things form across, you know, biological neural networks and artificial neural networks. So a famous example is vision models in the early layers, they have Gabor filters. And there's, you know, Gabor filters are something that neuroscientists are interested in and have thought a lot about. We find curve detectors in these models. Curve detectors are also found in monkeys. We discovered these high-low frequency detectors, and then some follow-up work went and discovered them in rats or mice. So they were found first in artificial neural networks and then found in biological neural networks. You know, there's this really famous result on like grandmother neurons or the Haley Berry neuron from Quiroga et al. And we found very similar things in vision models where this is while I was still at OpenAI and I was looking at their clip model. And you find these neurons that respond to the same entities in images. And also to give a concrete example, we found that there was a Donald Trump neuron. For some reason, I guess, everyone likes to talk about Donald Trump and Donald Trump was very prominent. It was a very hot topic at that time. So every neural network that we looked at, we would find a dedicated neuron for Donald Trump. And that was the only person who had always had a dedicated neuron. You know, sometimes you'd have an Obama neuron, sometimes you'd have a Clinton neuron, but Trump always had a dedicated neuron. So it responds to, you know, pictures of his face and the word Trump, like all of these things, right? And so it's not responding to a particular example or like it's not just responding to his face. It's abstracting over this general concept, right? So in any case, that's very similar to these queer-organized results. So there's evidence that this phenomenon of universality, the same things form across both artificial and natural neural networks. So that's a pretty amazing thing, if that's true. You know, it suggests that, well, I think the thing that it suggests is that gradient descent is sort of finding, you know, the right ways to cut things apart in some sense that many systems Converge on and many different neural networks architectures converge on. There's some natural set of, you know, there's some set of abstractions that are a very natural way to cut apart the problem and that a lot of systems are going to converge on. That would be my kind of, you know, I don't know anything about neuroscience. This is just my kind of wild speculation from what we've seen.
  Speaker 3
  Yeah, that would be beautiful if it's sort of agnostic to the medium of the model that's used to form the representation.
  Speaker 4
  Yeah, yeah. And it's, you know, it's a kind of a wild speculation based, you know, we only have a few data points that suggest this, but, you know, it does seem like there's some sense in which the same Things form again and again and again, both certainly in natural neural networks and also artificially or in biology.
  Speaker 3
  And the intuition behind that would be that, you know, words, in order to be useful in understanding the real world, you need all the same kind of stuff. Yeah. Well, if we pick, I don't know, like the idea of a dog, right?
  Speaker 4
  Like, you know, there's some sense in which the idea of a dog is like a natural category in the universe or something like this, right? Like, you know, there's some reason, it's not just like a weird quirk of like how humans factor, you know, think about the world that we have this concept of a dog. It's in some sense, or like if you have the idea of a line, like there's, you know, like look around us, you know, there are lines, you know, it's sort of the simplest way to understand this Room in some sense is to have the idea of a line.
  Speaker 3
  And so I think that that would be my instinct for why this happens. Yeah, you need a curved line, you know, to understand a circle, and you need all those shapes to understand bigger things. And yeah, it's a hierarchy of concepts that are formed. Yeah.
  Speaker 4
  And like, maybe there are ways to go and describe, you know, images without reference to those things, right? But they're not the simplest way or the most economical way or something like this. And so systems converge to these strategies would be my wild hypothesis.
  Speaker 3
  Can you talk through some of the building blocks that we've been referencing of features and circuits? So I think you first described them in a 2020 paper, Zoom In, An Introduction to Circuits. Absolutely. ([Time 4:29:19](https://share.snipd.com/snip/f04f002d-85be-4baa-b7ea-e0084aa9ea14))
    - **Note:** Multiversal truths in simplification of concepts in neural networks. Mechanistic interpretation is why models think what they do.
- Features and Circuits: The Essence of Neural Interpretation
  Summary:
  Understanding neural networks involves recognizing two key concepts: features and circuits.
  Features are interpreted as neuron-like entities that represent specific stimuli (e.g., detecting cars or dogs), while circuits refer to the connections between these features that collectively enable the detection and classification of complex objects. Not all neurons correspond directly to distinct concepts; many can represent combinations of items across layers of the model, indicating a superposition where features contribute to multiple interpretations.
  This leads to the formulation of the linear representation hypothesis, suggesting that the strength of neuron activations can be interpreted consistently, with more activity indicating higher confidence in detected entities.
  This hypothesis aligns with concepts seen in word embeddings, where relationships can be captured through vector arithmetic, revealing that directions in this mathematical space carry meaning.
  The idea remains robust across natural neural networks, although there have been discussions about deviations in smaller models. Scientific inquiry thrives on hypotheses, and the examination of the linear representation hypothesis underlines the utility of rigorous exploration before claiming absolutes, illustrating how theoretical frameworks evolve through challenge and validation.
  Transcript:
  Speaker 3
  Can you talk through some of the building blocks that we've been referencing of features and circuits? So I think you first described them in a 2020 paper, Zoom In, An Introduction to Circuits. Absolutely.
  Speaker 4
  So maybe I'll start by just describing some phenomena, and then we can sort of build to the idea of features and circuits. So if you spent like quite a few years, maybe like five years to some extent, with other things, studying this one particular model, Inception v1, which is this one vision model. It was state of the art in 2015 and, you know, very much not state of the art anymore. And it has, you know, maybe about 10,000 neurons in it. And I spent a lot of time looking at the 10,000 neurons, odd neurons of Inception v1. And one of the interesting things is, you know, there are lots of neurons that don't have some obvious interpolating meaning, but there's a lot of neurons in Inception v1 that do have Really clean interpolating meanings. So you find neurons that just really do seem to detect curves, and you find neurons that really do seem to detect cars, and car wheels, and car windows, and, you know, floppy ears of dogs, And dogs with long snouts facing to the right and dogs with long snouts facing to the left and different kinds of fur. There's sort of this whole beautiful edge detectors, line detectors, color contrast detectors, these beautiful things we call high-low frequency detectors. I think looking at it, I sort of felt like a biologist. You're looking at this sort of new world of proteins and you're discovering all these different proteins that interact. So one way you could try to understand these models is in terms of neurons. You could try to be like, oh, you know, there's a dog detecting neuron and here's a car detecting neuron. And it turns out you can actually ask how those connect together. So you can go and say, oh, you know, I have this car detecting neuron. How was it built? And it turns out in the previous layer, it's connected really strongly to a window detector and a wheel detector and a sort of car body detector. And it looks for the window above the car and the wheels below and the car chrome sort of in the middle, sort of everywhere, but especially on the lower part. And that's sort of a recipe for a car. Like that is, you know, earlier we said the thing we wanted from Mechantorp was to get algorithms, to go and get, you know, ask what is the algorithm that runs? Well, here we're just looking at the weights of the neuron that we're reading off this kind of recipe for detecting cars. It's a very simple crude recipe, but it's there. And so we call that a circuit, this connection. Well, okay. So the problem is that not all of the neurons are interval. And there's reason to think, we can get into this more later, that there's this superposition hypothesis. There's reason to think that sometimes the right unit to analyze things in terms of is combinations of neurons. So sometimes it's not that there's a single neuron that represents, say, a car, but it actually turns out after you detect the car, the model sort of hides a little bit of the car in the Following layer and a bunch of dog detectors. Why is it doing that? Well, you know, maybe it just doesn't want to do that much work on cars at that point. And, you know, it's sort of storing it away to go and... So it turns out then that this sort of subtle pattern of, you know, there's all these neurons that you think are dog detectors, and maybe they're primarily that, but they all a little Bit contribute to representing a car in that next layer. Okay. So now we can't really think there, there might still be some, something that you, I don't know, you could call it like a car concept or something, but it no longer corresponds to a neuron. So we need some term for these kind of neuron-like entities, these things that we sort of would have liked the neurons to be, these idealized neurons, the things that are the nice neurons, But also maybe there's more of them somehow hidden, and we call those features. And then what are circuits? So circuits are these connections of features, right? So when we have the car detector and it's connected to a window detector and a wheel detector, and it looks for the wheels below and the windows on top, that's a circuit. So circuits are just collections of features connected by weights, and they implement algorithms. So they tell us, you know, how are features used? How are they built? How do they connect together? So maybe it's worth trying to pin down, like, what really is the core hypothesis here? And I think the core hypothesis is something we call the linear representation hypothesis. So if we think about the car detector, you know, the more it fires, the more we sort of think of that as meaning, oh, the model is more and more confident that a car is present. Or, you know, if it's some combination of neurons that represent a car, you know, the more that combination fires, the more we think the model thinks there's a car present. This doesn't have to be the case, right? Like you could imagine something where you have, you know, you have this car detector on and you think, ah, you know, if it fires like, you know, between one and two, that means one thing, But it means like totally different if it's between three and four. That would be a nonlinear representation. And in principle that, you know, models could do that. I think it's sort of inefficient for them to do. If you try to think about how you'd implement computation like that, it's kind of an annoying thing to do. But in principle, models can do that. So one way to think about the features and circuits sort of framework for thinking about things is that we're thinking about things as being linear. We're thinking about there as being that if a neuron or a combination of neurons fires more, it's sort of that means more of a particular thing being detected. And then that gives weights a very clean interpretation as these edges between these entities, these features. And that edge then has a meaning. So that's in some ways the core thing. It's like, you know, we can talk about this sort of outside the context of neurons. Are you familiar with the Word2Vec results? So you have like, you know, minus man plus woman equals queen. Well, the reason you can do that kind of arithmetic is because you have a linear representation. Can you actually explain that representation a little bit? First of all, the feature is the direction of activation.
  Speaker 3
  Yeah, exactly. Can you do the minus men plus women, the word to VEC stuff? Can you explain what that is?
  Speaker 4
  It's such a simple, clean explanation of what we're talking about. Exact... ([Time 4:34:03](https://share.snipd.com/snip/5c778c37-e074-4267-8318-188167d6a363))
    - **Note:** How the brain may work. Linear representation of thinking. Mapping words to a vector.
- Harnessing the Power of Superposition in Neural Networks
  Summary:
  Recognizing the interplay between confidence and assumption is crucial in problem solving, where pushing boundaries within certain assumptions can lead to valuable insights.
  The superposition hypothesis suggests that neural networks can represent more concepts than the number of dimensions available in their architecture through a phenomenon called compressed sensing, where high-dimensional, sparse vectors can be projected into lower-dimensional spaces without losing essential information. This can allow for distinct concepts to coexist within fewer dimensions, helping justify the existence of polysemantic neuronal representations that respond to multiple, often unrelated stimuli.
  As such, these neurons do not simply adhere to one clean feature but can represent complex relationships that challenge traditional interpretations.
  Gradient descent, in this context, aids in optimizing this search for efficient representations amidst the sparse model landscapes.
  While the number of distinct concepts is likely bounded by the number of parameters, existing theorems indicate that the potential for coexistent, nearly orthogonal vectors exists, suggesting exponential growth in capability relative to neuron count. Addressing polysemanticity within neural networks becomes crucial for understanding specific functionalities of neurons and their interconnections. The extraction of monosemantic features from polysemantic networks through techniques such as dictionary learning, particularly via sparse autoencoders, offers a pathway for uncovering these structured meanings that naturally emerge, thus validating the efficacy of both linear representations and the superposition hypothesis in neural networks.
  Transcript:
  Speaker 4
  I think one wants to be able to pop up and sort of recognize the appropriate level of confidence. But I think there's also a lot of value in just being like, you know, I'm going to essentially assume, I'm going to condition on this problem being possible or this being broadly the right Approach. And I'm just going to go and assume that for a while and go and work within that and push really hard on it. And, you know, if society has lots of people doing that for different things, that's actually really useful in terms of going and getting to, getting, you know, either really ruling Things out, right? We can be like, well, you know, that didn't work. And we know that somebody tried hard, or going and getting to something that does teach us something about the world. So another interesting hypothesis is the superposition hypothesis. Can you describe what superposition is? Yeah. So earlier we were talking about word to back, right? And we were talking about how, you know, maybe you have one direction that corresponds to gender and maybe another that corresponds to royalty and another one that corresponds to Italy And another one that corresponds to, you know, food and all of these things. Well, you know, oftentimes maybe these word embeddings, they might be 500 dimensions, a thousand dimensions. And so if you believe that all of those directions were orthogonal, then you could only have, you know, 500 concepts. And, you know, I love pizza. But like, if I was going to go and like give the like 500 most important concepts in, you know, the English language, probably Italy wouldn't be, it's not obvious at least that Italy would Be one of them, right? Because you have to have things like plural and singular and verb and noun and adjective. And, you know, there's a lot of things we have to get to before we get to Italy and Japan. And, you know, there's a lot of countries in the world. And so how might it be that models could, you know, simultaneously have the linear representation hypothesis be true and also represent more things than they have directions. So what does that mean? Well, okay, so if linear representation hypothesis is true, something interesting has to be going on. Now, I'll tell you one more interesting thing before we go and we do that, which is, you know, earlier we were talking about all these polysomatic neurons, right? These neurons that, you know, when we're looking at Inception V1, there's these nice neurons that like the car detector and the curve detector and so on that respond to lots of, you know, To to very coherent things. But it's lots of neurons that respond to a bunch of unrelated things. And that's also an interesting phenomenon. And it turns out as well that even these neurons that are really, really clean, if you look at the weak activations, right? So if you look at like, you know, the activations where it's like activating 5% of the, you know, of the maximum activation, it's really not the core thing that it's expecting, right? So if you look at a curve detector, for instance, and you look at the places where it's 5% active, you know, you could interpret it just as noise, or it could be that it's doing something Else there. Okay, so how could that be? Well, there's this amazing thing in mathematics compressed sensing. And it's actually this very surprising fact where if you have a high dimensional space and you project it into a low dimensional space, ordinarily you can't go and sort of unproject It and get back your high dimensional vector, right? You threw information away. This is like, you know, you can't invert a rectangular matrix. You can only invert square matrices. But it turns out that that's actually not quite true. If I tell you that the high dimensional vector was sparse, so it's mostly zeros, then it turns out that you can often go and find back the high dimensional vector with very high probability. So that's a surprising fact, right? It says that, you know, you can have this high-dimensional vector space, and as long as things are sparse, you can project it down, you can have a lower-dimensional projection of it, And that works. So the Suitsich hypothesis is saying that that's what's going on in neural networks. For instance, that's what's going on in word embeddings. The word embeddings are able to simultaneously have directions be the meaningful thing. And by exploiting the fact that they're operating on a fairly high dimensional space, they're actually, and the fact that these concepts are sparse, right? Like, you know, you usually aren't talking about Japan and Italy at the same time. You know, most of those concepts, you know, in most sentences, Japan and Italy are both zero. They're not present at all. And if that's true, then you can go and have it be the case that you can have many more of these sort of directions that are meaningful, these features, than you have dimensions. And similarly, when we're talking about neurons, you can have many more concepts than you have neurons.. So that's the, at a high level of superstition hypothesis. Now it has this even wilder implication, which is to go and say that neural networks are, it may not just be the case that the representations are like this, but the computation may also Be like this, you know, the connections between all of them. And so in some sense, neural networks may be shadows of much larger, sparser neural networks. And what we see are these projections. And the strongest version of the superstition hypothesis would be to take that really seriously and sort of say, you know, there actually is in some sense this upstairs model, this, You know, where the neurons are really sparse and all-interpol, and there's, you know, the weights between them are these really sparse circuits. And that's what we're studying. And the thing that we're observing is the shadow of it, and so we need to find the original object.
  Speaker 3
  And the process of learning is trying to construct a compression of the upstairs that doesn't lose too much information in the projection.
  Speaker 4
  Yeah, it's finding how to fit it efficiently or something like this. The gradient descent is doing this. And in fact, so this sort of says that gradient descent, you know, it could just represent a dense neural network, but it sort of says that gradient descent is implicitly searching over ... ([Time 4:46:38](https://share.snipd.com/snip/b1ec430f-7b48-460d-b650-dc4aee4c1eff))
    - **Note:** Superposition theory of AI and the layers of neural networks that reduce and uncompress sparse complexity.

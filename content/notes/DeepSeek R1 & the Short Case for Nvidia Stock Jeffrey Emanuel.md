# DeepSeek R1 & the Short Case for Nvidia Stock | Jeffrey Emanuel

![](https://wsrv.nl/?url=https%3A%2F%2Fstatic.libsyn.com%2Fp%2Fassets%2Fc%2Ff%2Fd%2F4%2Fcfd431701301218b%2Fbankless-logo_1.png&w=100&h=100)

### Metadata

- Author: Bankless
- Full Title: DeepSeek R1 & the Short Case for Nvidia Stock | Jeffrey Emanuel
- Category: #podcasts



- URL: https://share.snipd.com/episode/691a57f0-37bb-4b17-983b-63b3e1ac00c1

### Highlights

- **DeepSeek's Efficiency Gains**
  - DeepSeek uses several clever tricks to achieve significant cost reductions.
  - One trick involves storing data in a compressed representation, similar to how attention works in the brain.
  - Another trick is multi-token prediction, allowing the model to process multiple words simultaneously and increasing throughput.
  - They also store model parameters in a compressed form and use lower precision throughout the training process, further improving efficiency.
  - These combined optimizations lead to substantial cost savings in memory, computation, and inter-GPU communication.
  Transcript:
  Ryan Sean Adams
  Maybe a simple way to explain this for listeners who want some extra help with that is it's just maybe closer to how your brain works with attention, where when you're applying attention Somewhere, you're not thinking about every single thing under the sun all at once. You're kind of focusing on what's necessary.
  Jeffrey Emanuel
  And I think maybe you can't go too far with the anthropomorphizing like attention. Attention in this context means a very specific thing.
  Ryan Sean Adams
  And it's not I don't think it's going to help people maybe i can't remember if i heard this in your article maybe a different one but um it's like if a house has you know 20 different rooms And lights are on in every single room even though a person is only in one room and this new model only keeps the lights on for the specific room that the person is in at that one given time It's's some loose, broad stroke pattern like that.
  Jeffrey Emanuel
  Sort of. I mean, it's basically like, instead of just naively storing this massive amount of like key value data that shows you like, it's basically like if you have like the word job, it's very Different if you say nice job versus I just got a new job or, you know, like, you know, are you going to be able to handle that job for me? Like, and it's like knowing. So like the word job has like a certain representation in the model, but that representation has to be altered depending on its context. That's what attention is about. And that means that for every word, every token, you have to really have to store lots of different things depending on the context. And that's why it takes up so much memory. And they're able to store that in a very efficient way using, you know, basically like just storing this sort of subset of the data in a compressed representation. So that's one thing they did that saved a lot of things. Another thing they did that's very smart is called like multi-token prediction. So like usually these models, they predict the next token, the next word basically, based on the preceding tokens or words. And, you know, one at a time. And so it kind of is this bottleneck. And they're like, well, what if we tried to do, let's say, two or three at a time? Now, the problem with that is you can't really predict the second token without knowing what the next token is. And so how can you start with the second token until you know the first token? But you can do what's called a speculative decoding. But your speculative decoding might be wrong, in which case you wasted your time computing that second token. But what they did is they got very good at guessing what that second one would be, such that 95% of the time they get it right. And so they basically, just from that, you can sort of double your throughput on inference because, and by the way, that's part of the reason why they're able to charge so little for their API is because that's about inference costs. And so they said that one trick let them almost double throughput for no additional cost, but just by, so that's a very clever trick they did. And then they did another very clever trick with just the, you know, these models are basically just a gigantic list of numbers, if you will, called the parameters of the model. And they figured out a way to store those parameters in a much more kind of compressed form. And like kind of normally the way these models are trained is they use more precision kind of, you can think of it almost as like more decimal places of accuracy, but it's not actually How it works, but it's just sort of close enough to understand conceptually. And then often what they do is once they then train the model that way to make it so that it can run on a cheaper GPU, they do what's called quantization, where they sort of then kind of truncate And round off the numbers a little bit. But that does hurt the accuracy or not the quality of the intelligence of the model. And what the DeepSeat guys did is they managed to, instead of having to train at a higher precision and then quantize to a lower precision at the end, they managed to figure out how to mostly Do the entire process end-to using the smaller representation. And again, it's like, it's one of these things where efficiency gains are, they pay for themselves so many times. Because it's like, not only do you use less memory, but the calculations go faster. And, you know, and then you don't need to do as much inter-GPU communication because there's less data. And so it's like it's these efficiency gains paid off in multiple different ways. And so that's another thing they did. I mean, there's just this whole laundry list of, like, little tricks and optimizations they did that when you add them all together and, and they're not additive, right? They're multiplicative. Like each thing, you know, if it double, if this thing doubles it and this one increases it by 40% and this one doubles it also, you're multiplying those multipliers, if you will. And that's how you can get this very big number, like 45 times, which by the way, we don't really know. You know, we don't know for sure. They could have lied about the number of GPU hours they use. You know, they, one thing is clear though, that they're, they are charging 95% less for inference. So either they're losing money on that they really can do at least the inference part much cheaper than, you know, we can here in the West because, yeah.
  Ryan Sean Adams
  That 95% less money for inference, I think, is really the sticker, the shocking number that is sending companies like Meta and OpenAI reeling.
  Jeffrey Emanuel
  Like Sam Altman had to put out a tweet. Tweet. No, actually, Meta was up, I think, because it's actually, look, on the one hand, it's bad for Meta in that they have spent so many billions of dollars on GPUs and they're paying so much Money to their team to, like, come up with the llama models and stuff like that. And then it sort of does make them look a little, like, foolish when these guys are able to beat them at their own game for, you know, on a shoestring. ([TimeÂ 0:53:29](https://share.snipd.com/snip/9c7cf126-c981-4099-a8e5-04137f76f720))
